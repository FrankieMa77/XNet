{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bodypart classification using a Siamese network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to serve as a walk through of the work done in bodypart classifiction. Due to the small size of the dataset available a simple convolutional neural network which learns to differentiate bodyparts achieves a very bad accuracy score. The method of using a Siamese architecture leverages the small size of the dataset by comparing the input image against training images to find which ones the input image to predict are most similar to. This task may not be able to be done efficiently with a very large dataset.\n",
    "\n",
    "More about the theory and practicalities behind this idea, and the code presented here, can be found in the Documentation in the 'Bodypart Classification' section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Keras functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Cropping2D,Convolution2D\n",
    "from keras.layers import BatchNormalization, Reshape, Layer, Dense\n",
    "from keras.layers import Activation, Flatten, Dense, Dropout\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "from keras import losses\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model architecture including the convolutional network and the euclidean distance metric for 'likeness' comparison in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_dim):\n",
    "    '''Base network to be shared for feature extraction'''\n",
    "    seq = Sequential()\n",
    "    seq.add(Convolution2D(16,(3,3), input_shape=input_sh, activation='relu'))\n",
    "    seq.add(MaxPooling2D(2,2))\n",
    "    seq.add(Convolution2D(32,(3,3), padding = 'same',activation='relu'))\n",
    "    seq.add(MaxPooling2D(2,2))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Convolution2D(64,(3,3), padding = 'same',activation='relu'))\n",
    "    seq.add(MaxPooling2D(2,2))\n",
    "    seq.add(Dropout(0.1))\n",
    "    seq.add(Flatten())\n",
    "    seq.add(Dense(32, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    '''Returns the euclidean distance in feature space'''\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    '''Returns the correct output shape for the network to compare to labels'''\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    margin = 1\n",
    "    return K.mean(y_true * K.square(y_pred) +\n",
    "                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel() < 0.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard hdf5 file created containing data places the different body parts in different classes. There are, however, no numerical labels for the data. Therefore, we write a function that labels the data with numbers for each bodypart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Label(array):\n",
    "    '''Assigns a numeric label to each image depending on bodypart'''\n",
    "    #Note: this function must be updated depending on the bodyparts available in the dataset\n",
    "    \n",
    "    label = np.array([])\n",
    "    for i in array:\n",
    "        if i == b'ankle':\n",
    "            label = np.append(label,0)\n",
    "        if i == b'arm':\n",
    "            label = np.append(label,1)\n",
    "        if i == b'femur':\n",
    "            label = np.append(label,2)\n",
    "        if i == b'foils':\n",
    "            label = np.append(label,3)\n",
    "        if i == b'hand':\n",
    "            label = np.append(label,4)\n",
    "        if i == b'knee':\n",
    "            label = np.append(label,5)\n",
    "        if i == b'leg':\n",
    "            label = np.append(label,6)\n",
    "        if i == b'lumbarspin':\n",
    "            label = np.append(label,7)\n",
    "        if i == b'neckoffemu':\n",
    "            label = np.append(label,8)\n",
    "        if i == b'cropped':\n",
    "            label = np.append(label,9)\n",
    "        #if i == b'shoulder':\n",
    "        #    label = np.append(label,9)\n",
    "        if i == b'thigh':\n",
    "            label = np.append(label,10)\n",
    "        if i == b'tibia':\n",
    "            label = np.append(label,11)\n",
    "        if i == b'wrist':\n",
    "            label = np.append(label,12)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we must pair up the data and assign each pair a label depending on whether each image in the pari belongs to the same or a different bodypart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(images,classes):\n",
    "    '''Pairs up images and assigns each pair a label'''\n",
    "    \n",
    "    no_images = len(images)\n",
    "    if no_images%2 != 0:\n",
    "        no_images -= 1\n",
    "    i = 0\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    while i < no_images:\n",
    "        pair_append = [[images[i],images[i+1]]]\n",
    "        labels_append = 0\n",
    "        if classes[i] != classes[i+1]:\n",
    "            labels_append = 1\n",
    "        pairs.append(pair_append)\n",
    "        labels.append(labels_append)\n",
    "        i = i + 2\n",
    "    pairs = np.asarray(pairs)\n",
    "    labels = np.asarray(labels)\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the path to the hdf5 data file\n",
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "hf = h5py.File(data_path, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = hf['train_img'][:]\n",
    "no_images, height,width, channels = train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = hf['train_bodypart'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = Label(train_classes)\n",
    "no_classes = len(np.unique(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, train_labels = format_data(train,train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = train_pairs.reshape(-1,2,height,width,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hf['test_img'][:]\n",
    "no_images, height,width, channels = test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = hf['test_bodypart'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = Label(test_classes)\n",
    "no_classes = len(np.unique(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs, test_labels = format_data(test,test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = test_pairs.reshape(-1,2,height,width,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = hf['val_img'][:]\n",
    "no_images, height,width, channels = val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classes = hf['val_bodypart'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_classes = Label(val_classes)\n",
    "no_classes = len(np.unique(val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs, val_labels = format_data(val,val_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pairs = val_pairs.reshape(-1,2,height,width,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model in Keras's ```Functional API```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the input shape\n",
    "input_sh = (height,width,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network definition\n",
    "base_network = create_base_network(input_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define input shapes to each of the two convolutional networks\n",
    "input_a = Input(shape=input_sh)\n",
    "input_b = Input(shape=input_sh)\n",
    "\n",
    "#Because we re-use the same instance `base_network`,\n",
    "#the weights of the network will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "#Find the distance in feature space \n",
    "distance = Lambda(euclidean_distance,\n",
    "                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n",
    "\n",
    "#Define the model\n",
    "model = Model([input_a, input_b], distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "model.compile(loss=contrastive_loss, optimizer=RMSProp(), metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model.fit([train_pairs[:, 0], train_pairs[:, 1]], train_labels,\n",
    "          batch_size=32,\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute final accuracy on training and test sets\n",
    "\n",
    "y_pred = model.predict([train_pairs[:, 0], train_pairs[:, 1]])\n",
    "tr_acc = compute_accuracy(train_labels, y_pred)\n",
    "y_pred = model.predict([test_pairs[:, 0], test_pairs[:, 1]])\n",
    "te_acc = compute_accuracy(test_labels, y_pred)\n",
    "y_pred = model.predict([val_pairs[:, 0], val_pairs[:, 1]])\n",
    "val_acc = compute_accuracy(val_labels, y_pred)\n",
    "\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n",
    "print('* Accuracy on validation set: %0.2f%%' % (100 * val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally the training should be done on an equal number of same and different pairings and the training iterations should alternate between different and same pairings. Both of these should help prevent overfitting to any one particular category. Similarly, bodypart pairings should be better randomised and intersperced in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prediction algorithm has not yet been built which compares the input 'to predict' image with all the training images, or at least one image from each bodypart class of training images, and compares it against each image. The maximum likelihood result can be taken for which the two images are the same and the bodypart class of the paired image with the most likely 'same' predicted label should be take to be the bodypart class of the 'to predict' image."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
